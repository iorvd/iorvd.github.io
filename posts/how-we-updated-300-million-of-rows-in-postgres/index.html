<!doctype html><html class="dark light"><head><meta charset=UTF-8><meta content="IE=edge" http-equiv=X-UA-Compatible><meta content="width=device-width,initial-scale=1.0" name=viewport><title>
         Updating 300 millions of rows in PostgreSQL
        
    </title><meta content="Updating 300 millions of rows in PostgreSQL" property=og:title><link href=https://ogesandr.com/fonts.css rel=stylesheet><link href=https://ogesandr.com/atom.xml rel=alternate title=ogesandr type=application/atom+xml><link href=https://ogesandr.com/theme/light.css rel=stylesheet><link href=https://ogesandr.com/theme/dark.css id=darkModeStyle rel=stylesheet><link href=https://ogesandr.com/main.css media=screen rel=stylesheet><body><div class=content><header><div class=main><a href=https://ogesandr.com>ogesandr</a><div class=socials><a class=social href=https://github.com/iorvd/ rel=me> <img alt=github src=/social_icons/github.svg> </a><a class=social href=/rss.xml rel=me> <img alt=rss src=/social_icons/rss.svg> </a></div></div><nav><a href=/posts style=margin-left:.7em>/posts</a><a href=/photos style=margin-left:.7em>/photos</a><a href=/about style=margin-left:.7em>/about</a> | <a href=javascript:void(0) id=dark-mode-toggle onclick=toggleTheme()> <img id=sun-icon src=/feather/sun.svg style=filter:invert()> <img id=moon-icon src=/feather/moon.svg> </a><script src=https://ogesandr.com/js/themetoggle.js></script></nav></header><main><article><div class=title><div class=page-header>Updating 300 millions of rows in PostgreSQL<span class=primary-color style=font-size:1.6em>.</span></div><div class=meta>Posted on <time>2024-09-11</time></div></div><section class=body><p>[<em>Originally posted at <a href=https://dou.ua/forums/topic/51676/>DOU.ua</a></em>]<p>This article was written as a retrospective of our experience migrating large amounts of data in PostgreSQL. Data migration is a fairly non-trivial task, and as our team discovered, in a resource-limited environment <code>UPDATE</code> of data is significantly more complex than <code>INSERT</code> or <code>DELETE</code>. We will review the preconditions and kind of data we migrated, the requirements set for the migration, explore the approaches we chose, and draw conclusions. This article will primarily be useful to those planning to perform large data migrations and those who want to deepen their knowledge of PostgreSQL.<h1 id=preconditions>Preconditions</h1><p>During a major refactoring of one of our services, we noticed some suboptimal data access patterns in in our database and decided to optimise them. Worth noting that this was not the only reason why we decided to perform the migration, but since this is all that can be revealed publicly, let's assume that all the reasons added up in a reasonable decision to migrate the data.<p>Let's look at the example that mimics the behavior we had. There are three entities — <code>user</code>, <code>transaction</code>, and <code>transaction_details</code>. The transaction details entity is linked to the transaction, and the transaction is linked to the user. So in order to get user data by transaction_details.id you first need to get the transaction detais, then get transaction, and only then get the data from the user. Our migration updated many fields, but for simplicity, we will consider in the query examples only updating the user_id field in the transaction_details table by transferring its value from the transactions table.<p><img alt=ERD src=https://s.dou.ua/storage-files/unnamed_B5Sn67S.png><p>A brief description of what we had and what our migration requirements were:<ul><li>The base in which migration was carried out was under constant load and served live traffic ~20rps with ocasional spikes up to 150rps. <img alt="Traffic HTTP" src=https://s.dou.ua/storage-files/unnamed_PndVs3L.png></ul><p style=text-align:center>http traffic database served<ul><li>We needed to reduce the load on the database as much as possible to avoid downtime of critical components of the app. At the same time we did not have strict restrictions on the migration execution time (provided that this did not require the use of engineers' working time). A faster solution was better for us than an ideal solution which would involve more human resources.<li>We did not have the opportunity to upgrade and then downgrade the database, but we added the maximum number of provisioned IOPS for our disk.<li>The fields that we planned to update (<code>user_id</code>) were immutable in the target-table (<code>transaction_details</code>). And once the row was created, they remained unchanged, so we did not need to worry about data races.<li>Zero data loss - we could not, for example, <a href=https://blog.codacy.com/how-to-update-large-tables-in-postgresql>stop the database</a>, migrate the data, and then start database again.</ul><h1 id=preparation>Preparation</h1><p>First, we started filling the newly created fields with data. This pinned the total number of rows across all the tables we needed to update to around 320 million, with the largest table containing 140 million records. Some of the configurational data we needed was in another database, but since the data we needed was immutable, and we were not interested in new records (because they were already automatically saturated with the data), in order to speed up the migration we completely copied the required table from one database to another.<h1 id=research-on-approaches-to-migration>Research on approaches to migration</h1><h2 id=aws-dms>AWS DMS</h2><p>We use AWS as a cloud provider for our services. One of AWS services is a service for data migration between databases — DMS (Data Migration Service). AWS DMS allows you to migrate data between homogeneous databases — Full Load and CDC, which consists of copying the main chunk of data in the database and then applying all changes that have occurred in the old database via CDC. During this migration, you can configure triggers that will modify the data — and as a result, you will get a ready copy of the original database, but with all the necessary changes.<p>Advantages:<ul><li>migration script execution speed.<li>if you plan to upgrade your database, you can perform both the upgrade and migration at the same time, saving time.</ul><p>Disadvantages:<ul><li>all components must be thoroughly tested, as there is a high probability of data loss at any migration stage or, conversely, the presence of data after migration that should have been deleted in the original table.<li>if we want to update the tables in several iterations, then each DMS migration will involve a significant amount of infrastructure team engineering resources, which were limited in our situation.<li>preparing to migrate all the tables at once is more difficult, since in the event of an error during the migration, it will have to be started from the beginning.</ul><h2 id=own-wheen-reinvented-dms>Own wheen-reinvented DMS</h2><p>If the approach of migrating the main chunk and adding data via CDC works, but there is no need to upgrade the database, then why use DMS if you can perform the same operations on the existing database? Let's consider an example of what such a migration could look like:<pre class=language-sql data-lang=sql style=color:#bfbab0;background-color:#0f1419><code class=language-sql data-lang=sql><span style=color:#5c6773;font-style:italic>-- remember the latest_updated_at;
</span><span style=color:#f73>SELECT</span><span> updated_at </span><span style=color:#f73>FROM</span><span> origin_table </span><span style=color:#f73>ORDER BY</span><span> updated_at </span><span style=color:#f73>DESC LIMIT </span><span style=color:#f29718>1</span><span>;
</span><span style=color:#5c6773;font-style:italic>-- copy the table either the following way or via pg_dump/pg_restore,
</span><span style=color:#5c6773;font-style:italic>-- and enrich the exported data locally;
</span><span style=color:#f73>CREATE TABLE </span><span>origin_table_copy </span><span style=color:#ffb454>FROM</span><span> (</span><span style=color:#f73>SELECT </span><span style=color:#39bae6;font-style:italic>* </span><span style=color:#f73>FROM</span><span> origin_table);
</span><span style=color:#5c6773;font-style:italic>-- create index on id;
</span><span style=color:#5c6773;font-style:italic>-- create trigger on create/update/delete, which will write from
</span><span style=color:#5c6773;font-style:italic>-- origin_table to origin_table_copy;
</span><span style=color:#5c6773;font-style:italic>-- incrementally restore the modified data;
</span><span style=color:#f73>UPDATE</span><span> origin_table_copy
</span><span style=color:#f73>SET</span><span> x</span><span style=color:#f29668>=</span><span>y
</span><span style=color:#f73>WHERE</span><span> updated_at </span><span style=color:#f29668>BETWEEN</span><span> latest_updated_at
</span><span>    </span><span style=color:#f29668>AND</span><span> latest_updated_at </span><span style=color:#f29668>+</span><span> (</span><span style=color:#f29718>10 </span><span style=color:#39bae6;font-style:italic>*</span><span> interval </span><span style=color:#c2d94c>'1 minute'</span><span>);
</span><span style=color:#5c6773;font-style:italic>-- restore all the constraints, recreate indexes, restart sequences etc;
</span><span style=color:#5c6773;font-style:italic>-- switch traffic between the tables;
</span></code></pre><p>Advantages:<ul><li>speed of migration script execution.<li>easier to replicate the migration environment, and as a result, easier to test individual components than in DMS<li>less involvement of infrastructure engineers<li>no problems with individual table migration.</ul><p>Disadvantages:<ul><li>at the same time, there are significantly more components to test than in DMS. Also, the logic of the migration itself is more complex, and accordingly, there is more room for a mistake.</ul><h2 id=batch-updates>Batch updates</h2><p>The idea behind updating data in batches is that it allows for a quick migration given the availability of database resources and a well-optimized query. In particular, this method is the easiest in terms of mental overhead for engineers, as it is intuitive in understanding. And therefore it helps to avoid mistakes.<p>Initially, we tried a fairly trivial and straightforward approach — to aggregate all the data in one sql transaction and immediately update the target table.<pre class=language-sql data-lang=sql style=color:#bfbab0;background-color:#0f1419><code class=language-sql data-lang=sql><span style=color:#f73>CREATE INDEX </span><span style=color:#ffb454>tx_details_user_id_null_idx
</span><span>    ON transaction_details (id)
</span><span>    </span><span style=color:#f73>WHERE</span><span> user_id </span><span style=color:#f29668>IS </span><span style=color:#f29718>NULL
</span><span>
</span><span style=color:#f73>UPDATE</span><span> transaction_details td
</span><span style=color:#f73>SET</span><span> user_id </span><span style=color:#f29668>= </span><span style=color:#f29718>cte</span><span>.</span><span style=color:#f29718>user_id
</span><span style=color:#f73>FROM</span><span> (</span><span style=color:#f73>SELECT </span><span style=color:#f29718>t</span><span>.</span><span style=color:#f29718>user_id</span><span>, </span><span style=color:#f29718>td</span><span>.</span><span style=color:#f29718>id
</span><span>      </span><span style=color:#f73>FROM</span><span> transactions t
</span><span>  </span><span style=color:#f73>JOIN</span><span> transaction_details td ON </span><span style=color:#f29718>td</span><span>.</span><span style=color:#f29718>transaction_id</span><span style=color:#f29668>=</span><span style=color:#f29718>t</span><span>.</span><span style=color:#f29718>id
</span><span>      </span><span style=color:#f73>WHERE </span><span style=color:#f29718>td</span><span>.</span><span style=color:#f29718>user_id </span><span style=color:#f29668>IS </span><span style=color:#f29718>NULL
</span><span>      </span><span style=color:#f73>LIMIT</span><span> _limit
</span><span>) </span><span style=color:#f29668>AS</span><span> cte
</span><span style=color:#f73>WHERE </span><span style=color:#f29718>td</span><span>.</span><span style=color:#f29718>id </span><span style=color:#f29668>= </span><span style=color:#f29718>cte</span><span>.</span><span style=color:#f29718>id</span><span>;
</span></code></pre><p>This worked very slowly, the speed was about 3k rows/min. Simple math suggests that at this rate, migrating one table would take <strong>~50 days</strong> (assuming we would turn the migration off at night). The first problem with the query above is that we waste the time JOINing the tables. To avoid this, we created an auxiliary table to which we added pre-aggregated data:<pre class=language-sql data-lang=sql style=color:#bfbab0;background-color:#0f1419><code class=language-sql data-lang=sql><span style=color:#5c6773;font-style:italic>-- join all the data we need and put it in a table
</span><span style=color:#f73>CREATE TABLE </span><span>tmp </span><span style=color:#ffb454>AS
</span><span style=color:#f73>SELECT</span><span> ROW_NUMBER() OVER() </span><span style=color:#f29668>AS</span><span> row_id,
</span><span>       </span><span style=color:#f29718>td</span><span>.</span><span style=color:#f29718>id </span><span style=color:#f29668>AS</span><span> td_id,
</span><span>       </span><span style=color:#f29718>t</span><span>.</span><span style=color:#f29718>id </span><span style=color:#f29668>AS</span><span> t_id
</span><span style=color:#f73>FROM</span><span> transaction_details td
</span><span style=color:#f73>JOIN</span><span> transactions t ON </span><span style=color:#f29718>td</span><span>.</span><span style=color:#f29718>transaction_id </span><span style=color:#f29668>= </span><span style=color:#f29718>t</span><span>.</span><span style=color:#f29718>id
</span><span style=color:#f73>WHERE </span><span style=color:#f29718>td</span><span>.</span><span style=color:#f29718>user_id </span><span style=color:#f29668>IS </span><span style=color:#f29718>NULL</span><span>;
</span><span>
</span><span style=color:#5c6773;font-style:italic>-- adding an index on top of it
</span><span style=color:#f73>CREATE INDEX </span><span>tmp_idx ON </span><span style=color:#ffb454>tmp</span><span>(row_id);
</span><span>
</span><span style=color:#5c6773;font-style:italic>-- table with all the data we need
</span><span style=color:#f73>UPDATE</span><span> transaction_details td
</span><span style=color:#f73>SET</span><span> user_id </span><span style=color:#f29668>=</span><span> COALESCE(</span><span style=color:#f29718>td</span><span>.</span><span style=color:#f29718>user_id</span><span>, </span><span style=color:#f29718>cte</span><span>.</span><span style=color:#f29718>user_id</span><span>)
</span><span style=color:#f73>FROM</span><span> (</span><span style=color:#f73>SELECT</span><span> user_id,
</span><span>             td_id
</span><span>      </span><span style=color:#f73>FROM</span><span> tmp
</span><span>      </span><span style=color:#f73>WHERE</span><span> row_id </span><span style=color:#f29668>> </span><span style=color:#f29718>200000 </span><span style=color:#f29668>AND</span><span> row_id </span><span style=color:#f29668><= </span><span style=color:#f29718>210000</span><span>) </span><span style=color:#f29668>AS</span><span> cte
</span><span style=color:#f73>WHERE </span><span style=color:#f29718>td</span><span>.</span><span style=color:#f29718>td_id </span><span style=color:#f29668>= </span><span style=color:#f29718>cte</span><span>.</span><span style=color:#f29718>td_id</span><span>;
</span></code></pre><p>The second problem we encountered was less obvious, but no less significant. The thing is that the <code>tx_details_user_id_null_idx</code> index does not speed up, but in fact slows down UPDATE, since when updating table rows, this index also needs to be updated. Thus, the advantages of the fast table reads are shadowed by the slow index updates. Therefore, we dropped it. After that the data was updating at ~5k rows/min rate, but we still were not satisfied with the speed.<p>We tried a few more rather bold ideas: vacuuming dead tuples after each iteration (it had almost no effect), playing with the postgres settings <code>temp_buffers</code>, <code>work_mem</code>, <code>effective_cache_size</code> (also minimal impact). In the end, after several iterations of changes, we came to the final version, the speed of which satisfied us <strong>~30k rows/min</strong>. Let's take a look at how we did it.<p>First, we created a table in which we aggregated all the data we need:<pre class=language-sql data-lang=sql style=color:#bfbab0;background-color:#0f1419><code class=language-sql data-lang=sql><span style=color:#5c6773;font-style:italic>-- join all the data we need and put it in a table
</span><span style=color:#f73>CREATE TABLE </span><span>tmp </span><span style=color:#ffb454>AS
</span><span style=color:#f73>SELECT </span><span style=color:#f29718>td</span><span>.</span><span style=color:#f29718>id         </span><span style=color:#f29668>AS</span><span> td_id,
</span><span>       </span><span style=color:#f29718>t</span><span>.</span><span style=color:#f29718>user_id         </span><span style=color:#f29668>AS</span><span> user_id,
</span><span>       </span><span style=color:#f29718>td</span><span>.</span><span style=color:#f29718>updated_at </span><span style=color:#f29668>AS</span><span> updated_at
</span><span style=color:#f73>FROM</span><span> transaction_details td
</span><span style=color:#f73>JOIN</span><span> transactions t ON </span><span style=color:#f29718>t</span><span>.</span><span style=color:#f29718>id </span><span style=color:#f29668>= </span><span style=color:#f29718>td</span><span>.</span><span style=color:#f29718>transaction_id
</span><span style=color:#f73>WHERE </span><span style=color:#f29718>td</span><span>.</span><span style=color:#f29718>user_id </span><span style=color:#f29668>IS </span><span style=color:#f29718>NULL</span><span>;
</span><span>
</span><span style=color:#5c6773;font-style:italic>-- create an index to read fast first N records and the delete them
</span><span style=color:#f73>CREATE INDEX </span><span>tmp_idx ON </span><span style=color:#ffb454>tmp</span><span> (updated_at, td_id) INCLUDE (user_id)
</span></code></pre><p>We found that an important aspect is sorting the index by updated_at. When updating data, our ideal scenario is to read all the data from the tables that will be updated in the same order in which they reside on disk. This allows us to reduce the read overhead on it. In PostgreSQL, the <code>UPDATE</code> operation is <code>INSERT+DELETE</code>, so when updating a row, its new versions can be written to a different memory region than the current one. As a result, the probability that rows with close <code>updated_at</code> values ​​are close to each other is higher than any two arbitrary rows sorted by UUID.<p>It is worth noting that this should be <code>updated_at</code>, not <code>created_at</code>. If the table <a href=https://www.postgresql.org/docs/16/sql-createtable.html#RELOPTION-FILLFACTOR>fillfactor</a> is 100% (the default value), then data updates with a high probability will be put on another table page, and the adjacent data risks be put far from each other after creation. We also tried sorting by ctid, but, strangely enough, it turned out to be much slower compared with updated_at.<p>When data is updated, indexes that reference the old version of that data are also updated. Therefore, it is a good idea to drop as many <a href=https://www.postgresql.org/docs/16/indexes-intro.html>indexes</a> of the updated table as possible and recreate them after the migration. We have observed ~10%-30% improvement in the speed of <code>UPDATE</code> operations after removing several large unused indexes.<p>For example, the query execution speed on a database clone with and without any indexes:<table><tr><th><th>With indexes<th>W/o indexes<tr><td>Batch size 1000<td>36 000<td>111 000</table><p>To reduce the overhead of updates, MVCC might use <a href=https://www.postgresql.org/docs/16/storage-hot.html>HOT</a> (Heap-Only-Tuples) optimization, but in order for it to happen the update must meet certain conditions:<ul><li>the update does not modify columns used in table indexes.<li>there is enough space on the table page that contains the old version of the row to fit the new version.</ul><p>And the second bullet above is the responsibility of the fillfactor - so if update-heavy workload dominates for a table it is worth thinking about creating tables with a fillfactor &LT70-90%. This will improve the speed of data updates during any migration as well.<p>At the end, our migration looked like this:<pre class=language-sql data-lang=sql style=color:#bfbab0;background-color:#0f1419><code class=language-sql data-lang=sql><span>DO $$
</span><span>    DECLARE
</span><span>        _id </span><span style=color:#f73>int</span><span> :</span><span style=color:#f29668>= </span><span style=color:#f29718>0</span><span>;
</span><span>        _rowsLimit </span><span style=color:#f73>INT</span><span> :</span><span style=color:#f29668>= </span><span style=color:#f29718>3000</span><span>;
</span><span>        _updatedTotal </span><span style=color:#f73>INT</span><span> :</span><span style=color:#f29668>= </span><span style=color:#f29718>0</span><span>;
</span><span>        _updatedInBatch </span><span style=color:#f73>INT</span><span>;
</span><span>        start_time </span><span style=color:#f73>timestamp</span><span> :</span><span style=color:#f29668>=</span><span> CLOCK_TIMESTAMP();
</span><span>        update_time </span><span style=color:#f73>INT</span><span>;
</span><span>        execution_time </span><span style=color:#f73>INT</span><span>;
</span><span>    </span><span style=color:#f73>BEGIN
</span><span>        </span><span style=color:#5c6773;font-style:italic>-- this setting is needed to turn off the trigger on the target table
</span><span>        </span><span style=color:#f73>SET</span><span> session_replication_role</span><span style=color:#f29668>=</span><span style=color:#c2d94c>'replica'</span><span>;
</span><span>
</span><span>        LOOP
</span><span>            RAISE NOTICE </span><span style=color:#c2d94c>'Started Iteration: %'</span><span>, _id;
</span><span>            </span><span style=color:#5c6773;font-style:italic>-- these rows from cte could be written into a variable and later
</span><span>            </span><span style=color:#5c6773;font-style:italic>-- reused for delete-operation, but we didn't think of it at the time,
</span><span>            </span><span style=color:#5c6773;font-style:italic>-- because delete was not our bottleneck even remotely.
</span><span>            </span><span style=color:#f73>WITH</span><span> cte </span><span style=color:#f29668>AS</span><span> (</span><span style=color:#f73>SELECT</span><span> td_id, user_id
</span><span>                         </span><span style=color:#f73>FROM</span><span> tmp
</span><span>                         </span><span style=color:#f73>ORDER BY</span><span> updated_at
</span><span>                         </span><span style=color:#f73>LIMIT</span><span> _rowsLimit
</span><span>            )
</span><span>            </span><span style=color:#f73>UPDATE</span><span> transaction_details td
</span><span>            </span><span style=color:#f73>SET</span><span> user_id </span><span style=color:#f29668>=</span><span> COALESCE(</span><span style=color:#f29718>td</span><span>.</span><span style=color:#f29718>user_id</span><span>, </span><span style=color:#f29718>cte</span><span>.</span><span style=color:#f29718>user_id</span><span>)
</span><span>            </span><span style=color:#f73>FROM</span><span> cte
</span><span>            </span><span style=color:#f73>WHERE </span><span style=color:#f29718>td</span><span>.</span><span style=color:#f29718>id </span><span style=color:#f29668>= </span><span style=color:#f29718>cte</span><span>.</span><span style=color:#f29718>td_id</span><span>;
</span><span>
</span><span>            </span><span style=color:#f73>COMMIT</span><span>;
</span><span>
</span><span>            GET DIAGNOSTICS _updatedInBatch </span><span style=color:#f29668>=</span><span> ROW_COUNT;
</span><span>            _updatedTotal :</span><span style=color:#f29668>=</span><span> _updatedTotal </span><span style=color:#f29668>+</span><span> _updatedInBatch;
</span><span>            update_time :</span><span style=color:#f29668>=</span><span> EXTRACT(EPOCH </span><span style=color:#f73>FROM</span><span> clock_timestamp() </span><span style=color:#f29668>-</span><span> start_time);
</span><span>            RAISE NOTICE </span><span style=color:#c2d94c>'UPDATE executed time: % sec.'</span><span>, update_time;
</span><span>
</span><span>            </span><span style=color:#f73>WITH</span><span> cte </span><span style=color:#f29668>AS</span><span> (</span><span style=color:#f73>SELECT</span><span> td_id, updated_at
</span><span>                         </span><span style=color:#f73>FROM</span><span> tmp
</span><span>                         </span><span style=color:#f73>ORDER BY</span><span> updated_at
</span><span>                         </span><span style=color:#f73>LIMIT</span><span> _rowsLimit
</span><span>            )
</span><span>            </span><span style=color:#f73>DELETE FROM</span><span> tmp t
</span><span>                </span><span style=color:#f73>USING</span><span> cte
</span><span>            </span><span style=color:#5c6773;font-style:italic>-- comparing `updated_at` is needed to force the query planner
</span><span>            </span><span style=color:#5c6773;font-style:italic>-- to use the index.
</span><span>            </span><span style=color:#f73>WHERE </span><span style=color:#f29718>t</span><span>.</span><span style=color:#f29718>updated_at </span><span style=color:#f29668>= </span><span style=color:#f29718>cte</span><span>.</span><span style=color:#f29718>updated_at
</span><span>              </span><span style=color:#f29668>AND </span><span style=color:#f29718>t</span><span>.</span><span style=color:#f29718>td_id </span><span style=color:#f29668>= </span><span style=color:#f29718>cte</span><span>.</span><span style=color:#f29718>td_id</span><span>;
</span><span>
</span><span>            execution_time :</span><span style=color:#f29668>=</span><span> EXTRACT(EPOCH </span><span style=color:#f73>FROM</span><span> clock_timestamp() </span><span style=color:#f29668>-</span><span> start_time);
</span><span>            RAISE NOTICE </span><span style=color:#c2d94c>'Finished Iteration: %, updated total: %, ALL time: % sec.'</span><span>, _id, _updatedTotal, execution_time;
</span><span>
</span><span>            </span><span style=color:#f73>COMMIT</span><span>;
</span><span>
</span><span>            IF _updatedInBatch </span><span style=color:#f29668>=</span><span> _rowsLimit </span><span style=color:#f73>THEN
</span><span>                PERFORM pg_sleep(</span><span style=color:#f29718>0</span><span>.</span><span style=color:#f29718>5</span><span>);
</span><span>            </span><span style=color:#f73>ELSE
</span><span>                RAISE NOTICE </span><span style=color:#c2d94c>'All IDs were updated. Exit.'</span><span>;
</span><span>                EXIT;
</span><span>            </span><span style=color:#f73>END</span><span> IF;
</span><span>
</span><span>            _id :</span><span style=color:#f29668>=</span><span> _id</span><span style=color:#f29668>+</span><span style=color:#f29718>1</span><span>;
</span><span>        </span><span style=color:#f73>END</span><span> LOOP;
</span><span>    </span><span style=color:#f73>END</span><span> $$;
</span></code></pre><h1 id=testing>Testing</h1><p>The query planner takes into account the state of the table and the database, so any query analysis should be performed on real data in the production database. Since there is always a risk of breaking something in production, we spinned up a clone of our production database and performed all hypothesis testing on it.<p>Additionally, we tested the migration segments via functional tests to make sure that they do exactly what we want them to do. Before executing the migration in production, we ran it in our test environments and verified that the data was migrated correctly.<h1 id=monitoring-and-performance-tuning>Monitoring and performance tuning</h1><p>When writing and executing migration, it is important to remember about domain-specific data access patterns.<ul><li>Are traffic spikes typical for services that work with the database? If so, you should consider reducing the speed of data migration.<li>Are there data synchronizations that create additional load on the database at night? Perhaps it is worth disabling migration at night.<li>How and when is data unloading for DWH or cache and can migration affect it? We had some CDC logic built on triggers - we had to disable it so as not to burden the services that use it (about 20 million rows were updated every day).<li>How long will it take to update one batch? Since thousands of values ​are being updated ​in the batch, each row will be locked with <code>FOR UPDATE</code> lock. Can this negatively affect other concurrent queries?<li>Are there any entities that use the database/table, the downtime of which directly affects the company's revenue? For us, such entities were endpoints that are responsible for conducting payments. During migration, it is important to identify such sensitive places and monitor whether requests are not falling due to timeout.<li>Who and how monitors the migration process and who sits near the kill switch script in case of an incident threat?<li>After migration, a significant number of dead tuples will be created and it is a good practice to clean them up using VACUUM ANALYZE.</ul><h1 id=conclusions>Conclusions</h1><p>Over time, you may need to re-saturate old data with new data or reshape the structure of tables, but as your database grows, this task becomes increasingly non-trivial. Increased query latency, disk degradation, data loss/data races — all of these problems appear to engineers in a new light when it comes to large amounts of data, and performing routine manipulations requires increasingly creative approaches and more time for preparation.<p>It is important to conduct more thorough and meticulous research on how you plan to perform the migration — from what script you will run to what metrics you will be able to look at if services dependent on the database begin to degrade.</section></article></main></div><script src=https://code.jquery.com/jquery-3.6.4.min.js></script><script src=https://cdn.jsdelivr.net/npm/nanogallery2@3.0.5/dist/jquery.nanogallery2.min.js></script><link href=https://cdn.jsdelivr.net/npm/nanogallery2@3.0.5/dist/css/nanogallery2.min.css rel=stylesheet><script>$(`.gallery`).each(function(a,b){let f=`onBottom`,n=`downloadButton, zoomButton, fullscreenButton, closeButton`,r=100,s=4,i=`fadeIn`,h=`right`,e=`auto`,q=`center`,l=800,o=2,d=1,m=`pageCounter`,k=30,g=`1em`,j=500,p=`fullContent`;const c=$(this)[0].children.length;if(c==d){$(this).nanogallery2({thumbnailHeight:e,thumbnailBorderHorizontal:d,thumbnailBorderVertical:d,thumbnailLabel:{display:!1,position:f,hideIcons:!0,titleFontSize:g,align:h,titleMultiLine:!0,displayDescription:!1},thumbnailDisplayTransition:i,thumbnailDisplayTransitionDuration:j,thumbnailDisplayInterval:k,touchAnimation:!0,touchAutoOpenDelay:l,locationHash:!1,viewerTools:{topLeft:m,topRight:n}})}else if(c==o){$(this).nanogallery2({galleryDisplayMode:p,thumbnailHeight:`400`,thumbnailAlignment:q,thumbnailBaseGridHeight:r,thumbnailL1GutterHeight:s,thumbnailBorderHorizontal:o,thumbnailBorderVertical:o,thumbnailLabel:{display:!1,position:f,hideIcons:!0,titleFontSize:g,align:h,titleMultiLine:!0,displayDescription:!1},thumbnailDisplayTransition:i,thumbnailDisplayTransitionDuration:j,thumbnailDisplayInterval:k,touchAnimation:!0,touchAutoOpenDelay:l,locationHash:!1,viewerTools:{topLeft:m,topRight:n}})}else{$(this).nanogallery2({galleryDisplayMode:p,thumbnailHeight:`250`,thumbnailWidth:e,thumbnailAlignment:q,thumbnailBaseGridHeight:r,thumbnailL1GutterHeight:s,thumbnailBorderHorizontal:o,thumbnailBorderVertical:o,thumbnailLabel:{display:!1,position:f,hideIcons:!0,titleFontSize:g,align:h,titleMultiLine:!0,displayDescription:!1},thumbnailDisplayTransition:i,thumbnailDisplayTransitionDuration:j,thumbnailDisplayInterval:k,touchAnimation:!0,touchAutoOpenDelay:l,locationHash:!1,viewerTools:{topLeft:m,topRight:n}})}})</script>